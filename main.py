# -*- coding: utf-8 -*-
import feedparser
import requests
import hashlib
import os
import re
import random
import subprocess
import json

from wand.image import Image
from wand.drawing import Drawing
from wand.color import Color

import arabic_reshaper
from bidi.algorithm import get_display

# ============================
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
# ============================
RSS_URL = "https://qenanews-24.blogspot.com/feeds/posts/default?alt=rss"
FONT_FILE = "29ltbukrabolditalic.otf"

BG_IMAGE = "BG.png"
LOGO_IMAGE = "logo1.png"

# Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù†Øµ
TEXT_LEFT = 110
TEXT_RIGHT = 960
TEXT_TOP = 725
TEXT_BOTTOM = 880

# Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§Ø­Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
MAX_WIDTH = TEXT_RIGHT - TEXT_LEFT  # 850px
MAX_HEIGHT = TEXT_BOTTOM - TEXT_TOP # 155px
CENTER_X = TEXT_LEFT + (MAX_WIDTH // 2)

POSTED_FILE = "posted_articles.txt"

PAGE_ID = os.getenv("PAGE_ID", "").strip()
PAGE_ACCESS_TOKEN = os.getenv("PAGE_ACCESS_TOKEN", "").strip()
FB_URL = f"https://graph.facebook.com/v19.0/{PAGE_ID}/photos"

# ============================
# ÙƒÙ„Ù…Ø§Øª Ø­Ø³Ø§Ø³Ø©
# ============================
SEPARATORS = ["$", "â€¢", "~", "+", "|", "=", "^", "!", "Â·", "âƒ"]

SENSITIVE_WORDS = [
    "Ù‚ØªÙ„","Ù…Ù‚ØªÙ„","Ù‚ØªÙŠÙ„","ÙŠÙ‚ØªÙ„","Ù‚ØªÙ„ØªÙ‡","Ø¬Ø±ÙŠÙ…Ø©","Ø¬Ø±Ø§Ø¦Ù…","Ù…Ø¬Ø±Ù…",
    "Ø°Ø¨Ø­","Ù…Ø°Ø¨ÙˆØ­","Ø·Ø¹Ù†","Ù…Ø·Ø¹ÙˆÙ†","Ø¶Ø±Ø¨","Ø§Ø¹ØªØ¯Ø§Ø¡","Ø§Ø¹ØªØ¯Ø§Ø¡Ø§Øª",
    "Ø¹Ù†Ù","ØªØ¹Ø°ÙŠØ¨","Ø¯Ù…","Ø¯Ù…Ø§Ø¡","Ù†Ø²ÙŠÙ","Ø³Ù„Ø§Ø­","Ø£Ø³Ù„Ø­Ø©","Ø³Ù„Ø§Ø­ Ø£Ø¨ÙŠØ¶",
    "Ø³ÙƒÙŠÙ†","Ù…Ø·ÙˆØ§Ø©","Ø¥Ø·Ù„Ø§Ù‚ Ù†Ø§Ø±","Ø±ØµØ§Øµ","Ø·Ù„Ù‚Ø§Øª","ØªÙØ¬ÙŠØ±","Ø§Ù†ÙØ¬Ø§Ø±",
    "Ù‚Ù†Ø¨Ù„Ø©","Ø§Ø®ØªØ·Ø§Ù","Ø®Ø·Ù","Ù…Ø®Ø·ÙˆÙ","Ø³Ø±Ù‚Ø©","Ø³Ø·Ùˆ","Ù†Ù‡Ø¨","ØªÙ‡Ø¯ÙŠØ¯","Ø§Ø¨ØªØ²Ø§Ø²",
    "ØªØ­Ø±Ø´","Ø§Ù„ØªØ­Ø±Ø´","ØªØ­Ø±Ø´ Ø¬Ù†Ø³ÙŠ","Ø§Ø¹ØªØ¯Ø§Ø¡ Ø¬Ù†Ø³ÙŠ","Ø§Ø¹ØªØ¯Ø§Ø¡Ø§Øª Ø¬Ù†Ø³ÙŠØ©",
    "Ø§ØºØªØµØ§Ø¨","Ù…ØºØªØµØ¨","Ù‡ØªÙƒ Ø¹Ø±Ø¶","Ø§Ù†ØªÙ‡Ø§Ùƒ","Ø§Ù†ØªÙ‡Ø§Ùƒ Ø¬Ø³Ø¯ÙŠ","Ø§Ø³ØªØºÙ„Ø§Ù„ Ø¬Ù†Ø³ÙŠ",
    "ØªØ­Ø±ÙŠØ¶ Ø¬Ù†Ø³ÙŠ","Ø·ÙÙ„Ø©","Ø·ÙÙ„","Ù‚Ø§ØµØ±","Ù‚Ø§ØµØ±Ø©","Ø§Ù„Ø§Ø¹ØªØ¯Ø§Ø¡ Ø¹Ù„Ù‰ Ø·ÙÙ„",
    "Ø§Ù„ØªØ­Ø±Ø´ Ø¨Ø§Ù„Ø£Ø·ÙØ§Ù„","Ø§Ø³ØªØºÙ„Ø§Ù„ Ø§Ù„Ø£Ø·ÙØ§Ù„","Ø§Ù†ØªØ­Ø§Ø±","Ø§Ù†ØªØ­Ø±","ÙŠÙ†ØªØ­Ø±",
    "Ø¥ÙŠØ°Ø§Ø¡ Ø§Ù„Ù†ÙØ³","Ø£Ø°Ù‰ Ø§Ù„Ù†ÙØ³","Ø´Ù†Ù‚","Ø´Ù†Ù‚ Ù†ÙØ³Ù‡","ØªÙ†Ø§ÙˆÙ„ Ø³ÙÙ…","Ø¬Ø±Ø¹Ø© Ø²Ø§Ø¦Ø¯Ø©",
    "Ø¥Ø±Ù‡Ø§Ø¨","Ø¥Ø±Ù‡Ø§Ø¨ÙŠ","ØªÙØ¬ÙŠØ± Ø¥Ø±Ù‡Ø§Ø¨ÙŠ","ØªÙ†Ø¸ÙŠÙ… Ø¥Ø±Ù‡Ø§Ø¨ÙŠ","Ø¯Ø§Ø¹Ø´","ØªÙØ¬ÙŠØ±Ø§Øª",
    "Ø¹Ù…Ù„ÙŠØ§Øª Ø¥Ø±Ù‡Ø§Ø¨ÙŠØ©","Ø¬Ù†Ø³","Ø¬Ù†Ø³ÙŠØ©","Ø¹Ù„Ø§Ù‚Ø© Ø¬Ù†Ø³ÙŠØ©","Ø¥Ø¨Ø§Ø­ÙŠØ©","Ù…ÙˆØ§Ø¯ Ø¥Ø¨Ø§Ø­ÙŠØ©",
    "Ù…Ù…Ø§Ø±Ø³Ø© Ø¬Ù†Ø³ÙŠØ©","Ø¹Ù†ØµØ±ÙŠØ©","ÙƒØ±Ø§Ù‡ÙŠØ©","Ø®Ø·Ø§Ø¨ ÙƒØ±Ø§Ù‡ÙŠØ©","ØªØ­Ø±ÙŠØ¶",
    "ØªØ­Ø±ÙŠØ¶ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ù","Ø³Ø¨","Ø¥Ù‡Ø§Ù†Ø©","ØªØ´Ù‡ÙŠØ±","Ù…Ø®Ø¯Ø±Ø§Øª","Ù…Ø®Ø¯Ø±","Ø­Ø´ÙŠØ´",
    "Ø¨Ø§Ù†Ø¬Ùˆ","Ù‡ÙŠØ±ÙˆÙŠÙ†","ÙƒÙˆÙƒØ§ÙŠÙŠÙ†","ØªØ±Ø§Ù…Ø§Ø¯ÙˆÙ„","ØªØ¹Ø§Ø·ÙŠ","ØªØ±ÙˆÙŠØ¬ Ù…Ø®Ø¯Ø±Ø§Øª",
    "ÙØ³Ø§Ø¯","Ø±Ø´ÙˆØ©","Ø§Ø®ØªÙ„Ø§Ø³","ØªØ²ÙˆÙŠØ±","ØªØ²ÙˆÙŠØ± Ø£ÙˆØ±Ø§Ù‚","ØºØ³ÙŠÙ„ Ø£Ù…ÙˆØ§Ù„"
]

def split_sensitive_word(word):
    if word not in SENSITIVE_WORDS:
        return word
    symbol = random.choice(SEPARATORS)
    pos = len(word) // 2
    return word[:pos] + symbol + word[pos:]

def process_sensitive_text(text, limit_once=False):
    words = text.split()
    used = False
    out = []
    for w in words:
        stripped_w = re.sub(r'[^\w]', '', w) 
        has_sensitive = any(s in w for s in SENSITIVE_WORDS)
        
        if has_sensitive and (not used or not limit_once):
            out.append(split_sensitive_word(w))
            used = True
        else:
            out.append(w)
    return " ".join(out)

# ============================
# Ø§Ù„Ø£Ù…Ø§ÙƒÙ† ÙˆØ§Ù„Ù‡Ø§Ø´ØªØ§Ø¬Ø§Øª
# ============================
PLACES = [
    "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©","Ø§Ù„Ø¬ÙŠØ²Ø©","Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©","Ø§Ù„Ø¯Ù‚Ù‡Ù„ÙŠØ©","Ø§Ù„Ø´Ø±Ù‚ÙŠØ©","Ø§Ù„Ù‚Ù„ÙŠÙˆØ¨ÙŠØ©",
    "ÙƒÙØ± Ø§Ù„Ø´ÙŠØ®","Ø§Ù„ØºØ±Ø¨ÙŠØ©","Ø§Ù„Ù…Ù†ÙˆÙÙŠØ©","Ø§Ù„Ø¨Ø­ÙŠØ±Ø©","Ø¯Ù…ÙŠØ§Ø·",
    "Ø¨ÙˆØ±Ø³Ø¹ÙŠØ¯","Ø§Ù„Ø¥Ø³Ù…Ø§Ø¹ÙŠÙ„ÙŠØ©","Ø§Ù„Ø³ÙˆÙŠØ³",
    "Ø§Ù„ÙÙŠÙˆÙ…","Ø¨Ù†ÙŠ Ø³ÙˆÙŠÙ","Ø§Ù„Ù…Ù†ÙŠØ§","Ø£Ø³ÙŠÙˆØ·","Ø³ÙˆÙ‡Ø§Ø¬","Ù‚Ù†Ø§","Ø§Ù„Ø£Ù‚ØµØ±","Ø£Ø³ÙˆØ§Ù†",
    "Ø§Ù„Ø¨Ø­Ø± Ø§Ù„Ø£Ø­Ù…Ø±","Ø§Ù„ÙˆØ§Ø¯ÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯","Ù…Ø·Ø±ÙˆØ­","Ø´Ù…Ø§Ù„ Ø³ÙŠÙ†Ø§Ø¡","Ø¬Ù†ÙˆØ¨ Ø³ÙŠÙ†Ø§Ø¡",
    "Ù…Ø¯ÙŠÙ†Ø© Ù‚Ù†Ø§","Ù…Ø±ÙƒØ² Ù‚Ù†Ø§","Ù†Ø¬Ø¹ Ø­Ù…Ø§Ø¯ÙŠ","Ù…Ø±ÙƒØ² Ù†Ø¬Ø¹ Ø­Ù…Ø§Ø¯ÙŠ",
    "Ø¯Ø´Ù†Ø§","Ù…Ø±ÙƒØ² Ø¯Ø´Ù†Ø§","Ù‚ÙØ·","Ù…Ø±ÙƒØ² Ù‚ÙØ·","Ù‚ÙˆØµ","Ù…Ø±ÙƒØ² Ù‚ÙˆØµ",
    "Ø£Ø¨Ùˆ ØªØ´Øª","Ù…Ø±ÙƒØ² Ø£Ø¨Ùˆ ØªØ´Øª","ÙØ±Ø´ÙˆØ·","Ù…Ø±ÙƒØ² ÙØ±Ø´ÙˆØ·",
    "Ù†Ù‚Ø§Ø¯Ø©","Ù…Ø±ÙƒØ² Ù†Ù‚Ø§Ø¯Ø©","Ø§Ù„ÙˆÙ‚Ù","Ù…Ø±ÙƒØ² Ø§Ù„ÙˆÙ‚Ù"
]

GOV_ENTITIES = ["Ø§Ù„Ù†ÙŠØ§Ø¨Ø© Ø§Ù„Ø¹Ø§Ù…Ø©","ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©","ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¹Ø¯Ù„","Ù…Ø­ÙƒÙ…Ø©","Ø§Ù„Ø´Ø±Ø·Ø©","Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø£Ù…Ù†ÙŠØ©"]

SECTIONS = {
    "Ù‚Ø¶Ø§Ø¦ÙŠ": ["Ù…Ø­ÙƒÙ…Ø©","Ø§Ù„Ù†ÙŠØ§Ø¨Ø©","Ø­ÙƒÙ…","Ù‚Ø¶Øª"],
    "Ø£Ù…Ù†ÙŠ": ["Ø§Ù„Ù‚Ø¨Ø¶","Ø§Ù„Ø£Ù…Ù†","Ø§Ù„Ø´Ø±Ø·Ø©","ØªÙØªÙŠØ´"],
    "ØªØ¹Ù„ÙŠÙ…ÙŠ": ["Ù…Ø¯Ø±Ø³","Ø·Ù„Ø§Ø¨","ØªØ¹Ù„ÙŠÙ…","Ù…Ø¯Ø±Ø³Ø©"],
    "Ø±ÙŠØ§Ø¶ÙŠ": ["Ù…Ø¨Ø§Ø±Ø§Ø©","Ù„Ø§Ø¹Ø¨","Ù†Ø§Ø¯ÙŠ","Ø¨Ø·ÙˆÙ„Ø©"]
}

def detect_section(text):
    for sec, keys in SECTIONS.items():
        for k in keys:
            if k in text:
                return sec
    return "Ø£Ø®Ø¨Ø§Ø±"

def normalize_hashtag(text):
    return text.replace(" ", "_")

def extract_safe_hashtags(text):
    tags = ["Ù‚Ù†Ø§24"]
    for p in PLACES:
        if p in text:
            tags.append(normalize_hashtag(p))
            break
    for g in GOV_ENTITIES:
        if g in text:
            tags.append(normalize_hashtag(g))
            break
    tags.append(normalize_hashtag(detect_section(text)))
    return " ".join(f"#{t}" for t in tags)

# ============================
# Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙØ§Ù Ø§Ù„Ù†Øµ Ø­Ø³Ø¨ Ø§Ù„Ø¨ÙƒØ³Ù„
# ============================
def wrap_text_pixel_based(text, drawing, canvas, max_width_px):
    words = text.split()
    lines = []
    current_line = []
    
    for word in words:
        test_line = current_line + [word]
        test_str = " ".join(test_line)
        reshaped_text = arabic_reshaper.reshape(test_str)
        bidi_text = get_display(reshaped_text)
        metrics = drawing.get_font_metrics(canvas, bidi_text)
        
        if metrics.text_width <= max_width_px:
            current_line = test_line
        else:
            if current_line:
                final_str = " ".join(current_line)
                lines.append(get_display(arabic_reshaper.reshape(final_str)))
            current_line = [word]
            
    if current_line:
        final_str = " ".join(current_line)
        lines.append(get_display(arabic_reshaper.reshape(final_str)))
        
    return lines

# ============================
# Ø¯Ø§Ù„Ø© Ù…Ù„Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Øµ Ù„Ù„Ù…Ø±Ø¨Ø¹
# ============================
def fit_text_dynamic(text, canvas):
    font_size = 60
    min_font = 20
    
    with Drawing() as draw:
        draw.font = FONT_FILE
        
        while font_size >= min_font:
            draw.font_size = font_size
            line_height = int(font_size * 1.3)
            lines = wrap_text_pixel_based(text, draw, canvas, MAX_WIDTH)
            total_text_height = len(lines) * line_height
            
            if total_text_height <= MAX_HEIGHT and len(lines) > 0:
                return lines, font_size, line_height
            
            font_size -= 2
            
    return lines, min_font, int(min_font * 1.3)

# ============================
# Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Ù…Ø¹Ø¯Ù„)
# ============================
def main():
    feed = feedparser.parse(RSS_URL)

    posted = []
    if os.path.exists(POSTED_FILE):
        with open(POSTED_FILE, "r", encoding="utf-8") as f:
            posted = f.read().splitlines()

    for entry in feed.entries:
        raw_title = re.sub("<.*?>", "", entry.title).strip()
        raw_summary = re.sub("<.*?>", "", entry.summary).strip()

        h = hashlib.md5(raw_title.encode("utf-8")).hexdigest()
        if h in posted:
            continue

        title = process_sensitive_text(raw_title, limit_once=True)
        summary = process_sensitive_text(raw_summary)

        first_50 = " ".join(summary.split()[:50])

        # === 1. ØªØ¬Ù‡ÙŠØ² Ø§Ù„ÙƒØ§Ø¨Ø´Ù† (Ø¨Ø¯ÙˆÙ† Ø±Ø§Ø¨Ø·) ===
        caption = (
            f"{first_50}...\n\n"
            f"Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙˆØ±Ø§Ø¨Ø· Ø§Ù„Ø®Ø¨Ø± ÙÙŠ Ø£ÙˆÙ„ ØªØ¹Ù„ÙŠÙ‚ ğŸ‘‡\n\n"
            f"{extract_safe_hashtags(raw_title)}"
        )

        # === 2. ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ (Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ§Ù„Ø¹Ù†ÙˆØ§Ù†) ===
        comment_text = (
            f"{title}\n"
            f"Ø§Ù„Ø®Ø¨Ø± ÙƒØ§Ù…Ù„ Ù‡Ù†Ø§ ğŸ‘‡\n"
            f"{entry.link}"
        )

        with Image(filename=BG_IMAGE) as canvas:
            try:
                match = re.search(r'<img[^>]+src="([^">]+)"', entry.summary)
                if match:
                    r = requests.get(match.group(1), timeout=10)
                    with Image(blob=r.content) as art:
                        art.transform(resize='855x460^')
                        art.extent(855, 460)
                        canvas.composite(art, 112, 185)
                else:
                    with Image(filename=LOGO_IMAGE) as logo:
                        logo.resize(855, 460)
                        canvas.composite(logo, 112, 185)
            except:
                pass

            lines, font_size, line_height = fit_text_dynamic(title, canvas)
            total_h = len(lines) * line_height
            start_y = TEXT_TOP + (MAX_HEIGHT - total_h) // 2 + int(font_size * 0.8)

            with Drawing() as draw:
                draw.font = FONT_FILE
                draw.font_size = font_size
                draw.fill_color = Color("black")
                draw.text_alignment = "center"
                
                current_y = TEXT_TOP + (MAX_HEIGHT - total_h) // 2 + int(line_height * 0.8)

                for line in lines:
                    draw.text(CENTER_X, current_y, line)
                    current_y += line_height

                draw(canvas)

            canvas.save(filename="final.png")

        # === Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù†Ø´Ø± ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚ ===
        try:
            with open("final.png", "rb") as img:
                # 1. Ù†Ø´Ø± Ø§Ù„Ø¨ÙˆØ³Øª
                res = requests.post(
                    FB_URL,
                    data={"access_token": PAGE_ACCESS_TOKEN, "caption": caption},
                    files={"source": img}
                )

            if res.status_code == 200:
                # 2. Ø¬Ù„Ø¨ ID Ø§Ù„Ø¨ÙˆØ³Øª Ù„Ù†Ø´Ø± Ø§Ù„ØªØ¹Ù„ÙŠÙ‚
                post_data = res.json()
                # Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ ÙŠÙƒÙˆÙ† Ø§Ù„Ù…Ø¹Ø±Ù post_id ÙˆØ£Ø­ÙŠØ§Ù†Ø§Ù‹ id Ø­Ø³Ø¨ Ø§Ù„Ø±Ø¯
                post_id = post_data.get("post_id") or post_data.get("id")
                
                print(f"âœ… ØªÙ… Ù†Ø´Ø± Ø§Ù„Ø¨ÙˆØ³Øª: {post_id}")

                if post_id:
                    # 3. Ù†Ø´Ø± Ø§Ù„ØªØ¹Ù„ÙŠÙ‚
                    comment_url = f"https://graph.facebook.com/v19.0/{post_id}/comments"
                    comment_res = requests.post(
                        comment_url,
                        data={
                            "access_token": PAGE_ACCESS_TOKEN,
                            "message": comment_text
                        }
                    )
                    if comment_res.status_code == 200:
                        print("ğŸ’¬ ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø¨Ù†Ø¬Ø§Ø­")
                    else:
                        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚: {comment_res.text}")

                # Ø­ÙØ¸ ÙˆØªØ­Ø¯ÙŠØ« Git
                with open(POSTED_FILE, "a", encoding="utf-8") as f:
                    f.write(h + "\n")

                subprocess.run(["git", "config", "--global", "user.name", "Bot"])
                subprocess.run(["git", "config", "--global", "user.email", "bot@github.com"])
                subprocess.run(["git", "add", POSTED_FILE])
                subprocess.run(["git", "commit", "-m", "update posted articles"], check=False)
                subprocess.run(["git", "push"], check=False)

                break
            else:
                print(f"âŒ ÙØ´Ù„ Ø§Ù„Ù†Ø´Ø±: {res.text}")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©: {e}")

if __name__ == "__main__":
    main()
