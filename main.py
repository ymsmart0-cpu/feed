# -*- coding: utf-8 -*-
import feedparser
import requests
import hashlib
import os
import re
import random
import subprocess
import json

from wand.image import Image
from wand.drawing import Drawing
from wand.color import Color

import arabic_reshaper
from bidi.algorithm import get_display

# ============================
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
# ============================
RSS_URL = "https://qenanews-24.blogspot.com/feeds/posts/default?alt=rss"
FONT_FILE = "29ltbukrabolditalic.otf"

BG_IMAGE = "BG.png"
LOGO_IMAGE = "logo1.png"

# Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù†Øµ
TEXT_LEFT = 110
TEXT_RIGHT = 960
TEXT_TOP = 725
TEXT_BOTTOM = 880

MAX_WIDTH = TEXT_RIGHT - TEXT_LEFT
MAX_HEIGHT = TEXT_BOTTOM - TEXT_TOP
CENTER_X = TEXT_LEFT + (MAX_WIDTH // 2)

POSTED_FILE = "posted_articles.txt"

PAGE_ID = os.getenv("PAGE_ID", "").strip()
PAGE_ACCESS_TOKEN = os.getenv("PAGE_ACCESS_TOKEN", "").strip()

# Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù†Ø´Ø± Ø§Ù„ØµÙˆØ± ÙÙŠ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©
FB_URL = f"https://graph.facebook.com/v19.0/{PAGE_ID}/photos"

# ============================
# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø© ÙˆØ§Ù„Ù‡Ø§Ø´ØªØ§Ø¬Ø§Øª
# ============================
SEPARATORS = ["$", "â€¢", "~", "+", "|", "=", "^", "!", "Â·", "âƒ"]
SENSITIVE_WORDS = [
    "Ù‚ØªÙ„","Ù…Ù‚ØªÙ„","Ù‚ØªÙŠÙ„","ÙŠÙ‚ØªÙ„","Ù‚ØªÙ„ØªÙ‡","Ø¬Ø±ÙŠÙ…Ø©","Ø¬Ø±Ø§Ø¦Ù…","Ù…Ø¬Ø±Ù…",
    "Ø°Ø¨Ø­","Ù…Ø°Ø¨ÙˆØ­","Ø·Ø¹Ù†","Ù…Ø·Ø¹ÙˆÙ†","Ø¶Ø±Ø¨","Ø§Ø¹ØªØ¯Ø§Ø¡","Ø§Ø¹ØªØ¯Ø§Ø¡Ø§Øª",
    "Ø¹Ù†Ù","ØªØ¹Ø°ÙŠØ¨","Ø¯Ù…","Ø¯Ù…Ø§Ø¡","Ù†Ø²ÙŠÙ","Ø³Ù„Ø§Ø­","Ø£Ø³Ù„Ø­Ø©","Ø³Ù„Ø§Ø­ Ø£Ø¨ÙŠØ¶",
    "Ø³ÙƒÙŠÙ†","Ù…Ø·ÙˆØ§Ø©","Ø¥Ø·Ù„Ø§Ù‚ Ù†Ø§Ø±","Ø±ØµØ§Øµ","Ø·Ù„Ù‚Ø§Øª","ØªÙØ¬ÙŠØ±","Ø§Ù†ÙØ¬Ø§Ø±",
    "Ù‚Ù†Ø¨Ù„Ø©","Ø§Ø®ØªØ·Ø§Ù","Ø®Ø·Ù","Ù…Ø®Ø·ÙˆÙ","Ø³Ø±Ù‚Ø©","Ø³Ø·Ùˆ","Ù†Ù‡Ø¨","ØªÙ‡Ø¯ÙŠØ¯","Ø§Ø¨ØªØ²Ø§Ø²",
    "ØªØ­Ø±Ø´","Ø§Ù„ØªØ­Ø±Ø´","ØªØ­Ø±Ø´ Ø¬Ù†Ø³ÙŠ","Ø§Ø¹ØªØ¯Ø§Ø¡ Ø¬Ù†Ø³ÙŠ","Ø§Ø¹ØªØ¯Ø§Ø¡Ø§Øª Ø¬Ù†Ø³ÙŠØ©",
    "Ø§ØºØªØµØ§Ø¨","Ù…ØºØªØµØ¨","Ù‡ØªÙƒ Ø¹Ø±Ø¶","Ø§Ù†ØªÙ‡Ø§Ùƒ","Ø§Ù†ØªÙ‡Ø§Ùƒ Ø¬Ø³Ø¯ÙŠ","Ø§Ø³ØªØºÙ„Ø§Ù„ Ø¬Ù†Ø³ÙŠ",
    "ØªØ­Ø±ÙŠØ¶ Ø¬Ù†Ø³ÙŠ","Ø·ÙÙ„Ø©","Ø·ÙÙ„","Ù‚Ø§ØµØ±","Ù‚Ø§ØµØ±Ø©","Ø§Ù„Ø§Ø¹ØªØ¯Ø§Ø¡ Ø¹Ù„Ù‰ Ø·ÙÙ„",
    "Ø§Ù„ØªØ­Ø±Ø´ Ø¨Ø§Ù„Ø£Ø·ÙØ§Ù„","Ø§Ø³ØªØºÙ„Ø§Ù„ Ø§Ù„Ø£Ø·ÙØ§Ù„","Ø§Ù†ØªØ­Ø§Ø±","Ø§Ù†ØªØ­Ø±","ÙŠÙ†ØªØ­Ø±",
    "Ø¥ÙŠØ°Ø§Ø¡ Ø§Ù„Ù†ÙØ³","Ø£Ø°Ù‰ Ø§Ù„Ù†ÙØ³","Ø´Ù†Ù‚","Ø´Ù†Ù‚ Ù†ÙØ³Ù‡","ØªÙ†Ø§ÙˆÙ„ Ø³ÙÙ…","Ø¬Ø±Ø¹Ø© Ø²Ø§Ø¦Ø¯Ø©",
    "Ø¥Ø±Ù‡Ø§Ø¨","Ø¥Ø±Ù‡Ø§Ø¨ÙŠ","ØªÙØ¬ÙŠØ± Ø¥Ø±Ù‡Ø§Ø¨ÙŠ","ØªÙ†Ø¸ÙŠÙ… Ø¥Ø±Ù‡Ø§Ø¨ÙŠ","Ø¯Ø§Ø¹Ø´","ØªÙØ¬ÙŠØ±Ø§Øª",
    "Ø¹Ù…Ù„ÙŠØ§Øª Ø¥Ø±Ù‡Ø§Ø¨ÙŠØ©","Ø¬Ù†Ø³","Ø¬Ù†Ø³ÙŠØ©","Ø¹Ù„Ø§Ù‚Ø© Ø¬Ù†Ø³ÙŠØ©","Ø¥Ø¨Ø§Ø­ÙŠØ©","Ù…ÙˆØ§Ø¯ Ø¥Ø¨Ø§Ø­ÙŠØ©",
    "Ù…Ù…Ø§Ø±Ø³Ø© Ø¬Ù†Ø³ÙŠØ©","Ø¹Ù†ØµØ±ÙŠØ©","ÙƒØ±Ø§Ù‡ÙŠØ©","Ø®Ø·Ø§Ø¨ ÙƒØ±Ø§Ù‡ÙŠØ©","ØªØ­Ø±ÙŠØ¶",
    "ØªØ­Ø±ÙŠØ¶ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ù","Ø³Ø¨","Ø¥Ù‡Ø§Ù†Ø©","ØªØ´Ù‡ÙŠØ±","Ù…Ø®Ø¯Ø±Ø§Øª","Ù…Ø®Ø¯Ø±","Ø­Ø´ÙŠØ´",
    "Ø¨Ø§Ù†Ø¬Ùˆ","Ù‡ÙŠØ±ÙˆÙŠÙ†","ÙƒÙˆÙƒØ§ÙŠÙŠÙ†","ØªØ±Ø§Ù…Ø§Ø¯ÙˆÙ„","ØªØ¹Ø§Ø·ÙŠ","ØªØ±ÙˆÙŠØ¬ Ù…Ø®Ø¯Ø±Ø§Øª",
    "ÙØ³Ø§Ø¯","Ø±Ø´ÙˆØ©","Ø§Ø®ØªÙ„Ø§Ø³","ØªØ²ÙˆÙŠØ±","ØªØ²ÙˆÙŠØ± Ø£ÙˆØ±Ø§Ù‚","ØºØ³ÙŠÙ„ Ø£Ù…ÙˆØ§Ù„"
]

def break_sensitive_inside_word(word):
    for sensitive in SENSITIVE_WORDS:
        if sensitive in word:
            symbol = random.choice(SEPARATORS)
            pos = len(sensitive) // 2
            broken = sensitive[:pos] + symbol + sensitive[pos:]
            return word.replace(sensitive, broken, 1)
    return word

def process_sensitive_text(text, limit_once=False):
    words = text.split(); used = False; result = []
    for w in words:
        has_sensitive = any(s in w for s in SENSITIVE_WORDS)
        if has_sensitive and (not used or not limit_once):
            result.append(break_sensitive_inside_word(w)); used = True
        else: result.append(w)
    return " ".join(result)

PLACES = ["Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©","Ø§Ù„Ø¬ÙŠØ²Ø©","Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ©","Ø³ÙˆÙ‡Ø§Ø¬","Ù‚Ù†Ø§","Ø§Ù„Ø£Ù‚ØµØ±","Ø£Ø³ÙˆØ§Ù†","Ù…Ø¯ÙŠÙ†Ø© Ù‚Ù†Ø§","Ù†Ø¬Ø¹ Ø­Ù…Ø§Ø¯ÙŠ","Ø¯Ø´Ù†Ø§","Ù‚ÙØ·","Ù‚ÙˆØµ","Ø£Ø¨Ùˆ ØªØ´Øª","ÙØ±Ø´ÙˆØ·","Ù†Ù‚Ø§Ø¯Ø©","Ø§Ù„ÙˆÙ‚Ù"]
GOV_ENTITIES = ["Ø§Ù„Ù†ÙŠØ§Ø¨Ø© Ø§Ù„Ø¹Ø§Ù…Ø©","ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©","Ù…Ø­ÙƒÙ…Ø©","Ø§Ù„Ø´Ø±Ø·Ø©"]

def extract_safe_hashtags(text):
    tags = ["Ù‚Ù†Ø§24"]
    for p in PLACES:
        if p in text: tags.append(p.replace(" ", "_")); break
    for g in GOV_ENTITIES:
        if g in text: tags.append(g.replace(" ", "_")); break
    return " ".join(f"#{t}" for t in tags)

# ============================
# Ø¯ÙˆØ§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù†ØµÙˆØµ
# ============================
def wrap_text_pixel_based(text, drawing, canvas, max_width_px):
    words = text.split(); lines = []; current_line = []
    for word in words:
        test_line = current_line + [word]
        reshaped = get_display(arabic_reshaper.reshape(" ".join(test_line)))
        if drawing.get_font_metrics(canvas, reshaped).text_width <= max_width_px:
            current_line = test_line
        else:
            if current_line: lines.append(get_display(arabic_reshaper.reshape(" ".join(current_line))))
            current_line = [word]
    if current_line: lines.append(get_display(arabic_reshaper.reshape(" ".join(current_line))))
    return lines

def fit_text_dynamic(text, canvas):
    font_size = 60; min_font = 20
    with Drawing() as draw:
        draw.font = FONT_FILE
        while font_size >= min_font:
            draw.font_size = font_size
            line_height = int(font_size * 1.3)
            lines = wrap_text_pixel_based(text, draw, canvas, MAX_WIDTH)
            if (len(lines) * line_height) <= MAX_HEIGHT and len(lines) > 0:
                return lines, font_size, line_height
            font_size -= 2
    return lines, min_font, int(min_font * 1.3)

# ============================
# Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# ============================
def main():
    feed = feedparser.parse(RSS_URL)
    posted = []
    if os.path.exists(POSTED_FILE):
        with open(POSTED_FILE, "r", encoding="utf-8") as f:
            posted = f.read().splitlines()

    for entry in feed.entries:
        raw_title = re.sub("<.*?>", "", entry.title).strip()
        h = hashlib.md5(raw_title.encode("utf-8")).hexdigest()
        if h in posted: continue

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ
        title = process_sensitive_text(raw_title, limit_once=True)
        summary = re.sub("<.*?>", "", entry.summary).strip()
        summary_processed = process_sensitive_text(summary)
        first_50 = " ".join(summary_processed.split()[:50])

        # Ø§Ù„ÙƒØ§Ø¨Ø´Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Ø¨Ø¯ÙˆÙ† Ø±Ø§Ø¨Ø·)
        caption = (
            f"{first_50}...\n\n"
            f"Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙˆØ±Ø§Ø¨Ø· Ø§Ù„Ø®Ø¨Ø± ÙÙŠ Ø£ÙˆÙ„ ØªØ¹Ù„ÙŠÙ‚ ğŸ‘‡\n\n"
            f"{extract_safe_hashtags(raw_title)}"
        )
        
        # Ù†Øµ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ (Ø§Ù„Ø¹Ù†ÙˆØ§Ù† + Ø§Ù„Ø±Ø§Ø¨Ø·)
        comment_text = f"{title}\nØ§Ù„Ø®Ø¨Ø± ÙƒØ§Ù…Ù„ Ù‡Ù†Ø§ ğŸ‘‡\n{entry.link}"

        with Image(filename=BG_IMAGE) as canvas:
            try:
                match = re.search(r'<img[^>]+src="([^">]+)"', entry.summary)
                if match:
                    r = requests.get(match.group(1), timeout=10)
                    with Image(blob=r.content) as art:
                        art.transform(resize='855x460^'); art.extent(855, 460)
                        canvas.composite(art, 112, 185)
                else:
                    with Image(filename=LOGO_IMAGE) as logo:
                        logo.resize(855, 460); canvas.composite(logo, 112, 185)
            except: pass

            lines, font_size, line_height = fit_text_dynamic(title, canvas)
            total_h = len(lines) * line_height
            current_y = TEXT_TOP + (MAX_HEIGHT - total_h) // 2 + int(line_height * 0.8)

            with Drawing() as draw:
                draw.font = FONT_FILE; draw.font_size = font_size
                draw.fill_color = Color("black"); draw.text_alignment = "center"
                for line in lines:
                    draw.text(CENTER_X, current_y, line)
                    current_y += line_height
                draw(canvas)
            canvas.save(filename="final.png")

        # Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒ
        try:
            with open("final.png", "rb") as img:
                # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù„ØªØ¬Ù†Ø¨ Ø®Ø·Ø£ (#200)
                payload = {
                    "caption": caption,
                    "access_token": PAGE_ACCESS_TOKEN
                }
                files = {"source": img}
                res = requests.post(FB_URL, data=payload, files=files)
            
            if res.status_code == 200:
                data = res.json()
                post_id = data.get("post_id") or data.get("id")
                print(f"âœ… ØªÙ… Ø§Ù„Ù†Ø´Ø± Ø¨Ù†Ø¬Ø§Ø­: {post_id}")
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
                if post_id:
                    comment_url = f"https://graph.facebook.com/v19.0/{post_id}/comments"
                    c_res = requests.post(comment_url, data={
                        "message": comment_text,
                        "access_token": PAGE_ACCESS_TOKEN
                    })
                    if c_res.status_code == 200: print("ğŸ’¬ ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø¨Ù†Ø¬Ø§Ø­")
                    else: print(f"âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚: {c_res.text}")

                # Ø­ÙØ¸ Ø§Ù„Ø­Ø§Ù„Ø© ÙˆØªØ­Ø¯ÙŠØ« GitHub
                with open(POSTED_FILE, "a", encoding="utf-8") as f: f.write(h + "\n")
                subprocess.run(["git", "config", "--global", "user.name", "Bot"])
                subprocess.run(["git", "config", "--global", "user.email", "bot@github.com"])
                subprocess.run(["git", "add", POSTED_FILE])
                subprocess.run(["git", "commit", "-m", "update posted articles"], check=False)
                subprocess.run(["git", "push"], check=False)
                break
            else:
                print(f"âŒ ÙØ´Ù„ Ø§Ù„Ù†Ø´Ø±: {res.text}")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£: {e}")

if __name__ == "__main__":
    main()
